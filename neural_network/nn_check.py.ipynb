{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_activation(x):\n",
    "        return max(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return (1 - np.exp(x)) / (1 + np.exp(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        x_new = [np.exp(i) for i in x]\n",
    "        sum_x_new = sum(x_new)\n",
    "        return [sum_x_new / (i) for i in x_new]\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_relu(x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_sigmoid(x):\n",
    "        return (Activation.sigmoid(x)) * (1 - Activation.sigmoid(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_tanh(x):\n",
    "        return - np.exp(x) / (1 + np.exp(x)) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy(Y_pred, Y_train):\n",
    "        if Y_pred == 1:\n",
    "            return -np.log(Y_train)\n",
    "        else:\n",
    "            return -np.log(1 - Y_train)\n",
    "\n",
    "    @staticmethod\n",
    "    def hinge_loss(Y_pred, Y_train):\n",
    "        return np.max(0, 1 - Y_pred * Y_train)\n",
    "\n",
    "    @staticmethod\n",
    "    def L1_loss(Y_pred, Y_train):\n",
    "        return np.sum(np.absolute(Y_pred - Y_train))\n",
    "\n",
    "    @staticmethod\n",
    "    def L2_loss(Y_pred, Y_train):\n",
    "        return np.sum(np.power((Y_pred - Y_train), 2)) / len(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "        def __init__(self, train_x, train_y, hidden_layer=1, hidden_neurons=3, bias = 1):\n",
    "            self.train_x = train_x\n",
    "            self.train_y = train_y\n",
    "            \n",
    "            self.hidden_layer = hidden_layer\n",
    "            self.hidden_neurons = hidden_neurons\n",
    "            self.input_nodes = np.shape(train_x)[0]\n",
    "            self.output_nodes = np.shape(train_y)[0]\n",
    "            \n",
    "            self.bias = bias\n",
    "\n",
    "            self.loop_counter = self.hidden_layer + 1\n",
    "            # seed for the fixed random values\n",
    "            np.random.seed(3)\n",
    "            self.W_in = np.random.normal(0.0, 0.1, (self.input_nodes, self.hidden_neurons))\n",
    "            self.W_out = np.random.normal(0.0, 0.1, (self.hidden_neurons, self.output_nodes))\n",
    "            self.bias_weight_Mat = np.random.normal(0.0, 0.1, (1,self.loop_counter))\n",
    "            \n",
    "            # special case to check, override bias_weight_Mat\n",
    "            self.bias_weight_Mat = [0.35, 0.65]          \n",
    "            print(self.W_in, self.W_out, \"First and last weight mat\")\n",
    "            self.Weight_Mat = list()\n",
    "            self.Weight_Mat.append(self.W_in)\n",
    "            \n",
    "            if self.loop_counter > 2:\n",
    "                for i in range(1, self.loop_counter-1):\n",
    "                    self.Wh_i = np.random.normal(0.0, 0.1, (self.hidden_neurons, self.hidden_neurons))\n",
    "                    self.Weight_Mat.append(self.Wh_i)\n",
    "            self.Weight_Mat.append(self.W_out)\n",
    "             \n",
    "                    \n",
    "        def L2_loss(self, Y_pred, Y_train):\n",
    "            return np.sum(np.power((Y_pred - Y_train), 2)) / len(Y_train)\n",
    "        \n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        def forward_prop(self):\n",
    "            weight = self.Weight_Mat\n",
    "            # Layewise activation output\n",
    "            lw_net = list()\n",
    "            lw_act_out = list()\n",
    "            \n",
    "            for i in range(len(weight)):\n",
    "                print((weight[i]), i)\n",
    "                print((self.train_x), i)\n",
    "                X_i = np.dot(np.array(self.train_x).T, np.array(weight[i])) + self.bias_weight_Mat[i]\n",
    "                lw_net.append(X_i)\n",
    "                A_i = self.sigmoid(X_i)\n",
    "                lw_act_out.append(A_i)\n",
    "                self.train_x = (A_i).T\n",
    "            E_total = 0\n",
    "            \n",
    "            for i in range(len(A_i)):\n",
    "                E = self.L2_loss(np.array(A_i[i]), np.array(self.train_y[i]))\n",
    "                E_total += E\n",
    "            print(\"lw\", lw_net)\n",
    "            print(\"lw_ac_out\", lw_act_out)\n",
    "            print(\"total\", E_total)\n",
    "            return lw_net, lw_act_out, E_total\n",
    "            \n",
    "        \n",
    "#       backpropagation\n",
    "        def back_prop(self):\n",
    "            lw_net, lw_act_out, E_total = self.forward_prop()\n",
    "            \n",
    "#             for i in reversed(range(self.loop_counter)):\n",
    "#                 for j in range()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17886285  0.04365099  0.00964975]\n",
      " [-0.18634927 -0.02773882 -0.0354759 ]] [[-0.00827415 -0.06270007]\n",
      " [-0.00438182 -0.0477218 ]\n",
      " [-0.13138648  0.08846224]] First and last weight mat\n",
      "[[ 0.17886285  0.04365099  0.00964975]\n",
      " [-0.18634927 -0.02773882 -0.0354759 ]] 0\n",
      "[[0.5], [0.1]] 0\n",
      "[[-0.00827415 -0.06270007]\n",
      " [-0.00438182 -0.0477218 ]\n",
      " [-0.13138648  0.08846224]] 1\n",
      "[[0.60367383]\n",
      " [0.59122979]\n",
      " [0.58692728]] 1\n",
      "lw [array([[0.4207965 , 0.36905161, 0.35127728]]), array([[0.56530015, 0.63585596]])]\n",
      "lw_ac_out [array([[0.60367383, 0.59122979, 0.58692728]]), array([[0.637678  , 0.65381609]])]\n",
      "total 0.23715040802471377\n",
      "([array([[0.4207965 , 0.36905161, 0.35127728]]), array([[0.56530015, 0.63585596]])], [array([[0.60367383, 0.59122979, 0.58692728]]), array([[0.637678  , 0.65381609]])], 0.23715040802471377)\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([[0.5], [0.1]], [[0.99], [0.01]], 1, 3)\n",
    "print(nn.forward_prop())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
