{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, most of the time we do feature engineering. Feature engineering means thinking about types of independent variables which can used for building a model. In mathematical terms, see the following equation.\n",
    "\n",
    "                        Z = f(x,y,w)  --- (1)\n",
    "                        \n",
    "In the above equation, Z is a function of three variables namely x, y, w. This Z is partial dependence on each variable. Any change in x or y or z would bring some change in Z. \n",
    "\n",
    "What if y = g(x) and w = h(x)?\n",
    "\n",
    "If y, w are the function of x then x, y, w are not linearly independent features. In that case, then the above function Z would totally depend on x. Hence any change in y and z would would reflect the similar change as by changing x.\n",
    "\n",
    "In feature engineering, our ultimate goal is find the linearly independent features. The linearly independent features are useful for building a good machine learning model. \n",
    "\n",
    "Some statistical technique are available which can tell us the importance of these features. Principal component anlaysis is one of them. Let's have a look at PCA.\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Take an input matrix X as shown below.\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "1.0 & 6.0 & 0.0\\\\\n",
    "2.0 & 7.0 & 3.0\\\\\n",
    "3.0 & 2.0 & 9.0\\\\\n",
    "4.0 & 4.0 & 7.0\\\\\n",
    "5.0 & 0.0 & 6.0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Each column of the above matrix is a feature. Finding linear dependence becomes cumbersome if the number of columns grows to let's say 5000 or more. In that case PCA is the saviour. It gives you the set of independent feature sets which are orthogonal and linearly independent.\n",
    "\n",
    "How to calculate those feature component? Let's see the step by step process.\n",
    "\n",
    "Step 1. Calculate the covariance matrix of the given input matrix. To find the covariance matrix you need to calculate the mean for each column which together constitute a mean-vector.\n",
    "\n",
    "Step 2. Generate new matrix by subtracting the column specific mean from the elements of each column.\n",
    "\n",
    "Step.3 Covariance_Matrix = numpy.dot(X_mean0, X_mean0.T)/(number of element in column -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_mat(X):\n",
    "        \"\"\"Objects of this class is a Principal component.\n",
    "\n",
    "       Examples\n",
    "       --------\n",
    "       >>> import numpy as np\n",
    "       >>> X = np.array([[1,2,3,4,5], [2,3,6,9,0], [8,1,6,4,3]])\n",
    "\n",
    "       Parameter\n",
    "       -------------\n",
    "       X : A numpy array\n",
    "       \"\"\"\n",
    "        row_X, size_row_X = np.shape(X)\n",
    "        X_mean0 = np.empty(shape=np.shape(X))\n",
    "        temp_list = list()\n",
    "\n",
    "        \"\"\"\n",
    "         Variables:\n",
    "         ---------\n",
    "         row_X: number of features i.e., the number of lists in input matrix X\n",
    "         size_row_X: number of elements in each each list\n",
    "         X_mean0 : The transformed X when subtracted from mean\n",
    "        \"\"\"\n",
    "        for i in range(row_X):\n",
    "            mean_i = (np.sum(X[i]))/size_row_X\n",
    "            for j in X[i]:\n",
    "                j = j - mean_i\n",
    "                temp_list.append(j)\n",
    "            X_mean0[i] = temp_list\n",
    "            temp_list = []\n",
    "        cov_matrix = np.dot(X_mean0, X_mean0.T)/(row_X - 1)\n",
    "        return cov_matrix\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have the covariance matrix and you are ready to compute the principle components. The numpy library has linalgebra package which can find pca for you. You just have to pass the covariance matrix to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component(X):\n",
    "        Y = PCA.cov_mat(X)\n",
    "        eigen_val, eigen_vec = np.linalg.eig(Y)\n",
    "        \"\"\" eigenvalue and eigen vectors are extracted through a\n",
    "         square matrix which is the covariance matrix in our case.\n",
    "         Variables:\n",
    "         ---------\n",
    "         eigen_val: eignevalue of the input data X\n",
    "         eigen_vec: eignenvector of the input data X\n",
    "        \"\"\"\n",
    "        maximum = np.max(eigen_val)\n",
    "        index_pc = eigen_val.tolist().index(maximum)\n",
    "        return eigen_vec[index_pc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list of eigenvalues, I have extracted the maximum. The top priority is given to the eigenvectors having highest eigenvalues. I have chosen here only one. \n",
    "\n",
    "Let's say for a given input data, you got 10 eigenvectors. You want to train your system on 7 features. Then you will extract the top seven eigenvectors (i.e., seven eigenvectors corresponding to top seven eigenvalues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
